{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "You will need an image dataset to experiment with, as well as a few Python packages.\n",
    "\n",
    "# A Dataset to Play With\n",
    "We will be using the Canadian Institute for Advanced Research image dataset, better known as [CIFAR-10](https://en.wikipedia.org/wiki/CIFAR-10), which consists of 60,000 32x32 pixel color images belonging to different object classes, such as dogs, cats, and airplanes. Relatively, CIFAR is not a very large dataset, but if we were to use the full [TinyImages](https://groups.csail.mit.edu/vision/TinyImages/) dataset, then you would need about 400GB of free disk space, which would probably be a limiting factor.\n",
    "\n",
    "Credits for the dataset as described in [chapter 3 of this tech report](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf) go to Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
    "\n",
    "If you’d like to follow along with the code examples in this article, you can [download CIFAR-10 here](https://www.cs.toronto.edu/~kriz/cifar.html), selecting the Python version. You’ll be sacrificing 163MB of disk space:\n",
    "\n",
    "When you download and unzip the folder, you’ll discover that the files are not human-readable image files. They have actually been serialized and saved in batches using [cPickle](https://docs.python.org/2/library/pickle.html).\n",
    "\n",
    "While we won’t consider [pickle](https://realpython.com/python-pickle-module/) or cPickle in this article, other than to extract the CIFAR dataset, it’s worth mentioning that the Python pickle module has the key advantage of being able to serialize any Python object without any extra code or transformation on your part. It also has a potentially serious disadvantage of posing a security risk and not coping well when dealing with very large quantities of data.\n",
    "\n",
    "The following code unpickles each of the five batch files and loads all of the images into a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CIFAR-10 training set:\n",
      " - np.shape(images)     (50000, 32, 32, 3)\n",
      " - np.shape(labels)     (50000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the unzipped CIFAR data\n",
    "data_dir = Path(\"../../../data/cifar-10/cifar-10-batches-py/\")\n",
    "\n",
    "# Unpickle function provided by the CIFAR hosts\n",
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dict\n",
    "\n",
    "images, labels = [], []\n",
    "for batch in data_dir.glob(\"data_batch_*\"):\n",
    "    batch_data = unpickle(batch)\n",
    "    for i, flat_im in enumerate(batch_data[b\"data\"]):\n",
    "        im_channels = []\n",
    "        # Each image is flattened, with channels in order of R, G, B\n",
    "        for j in range(3):\n",
    "            im_channels.append(\n",
    "                flat_im[j * 1024 : (j + 1) * 1024].reshape((32, 32))\n",
    "            )\n",
    "        # Reconstruct the original image\n",
    "        images.append(np.dstack((im_channels)))\n",
    "        # Save the label\n",
    "        labels.append(batch_data[b\"labels\"][i])\n",
    "\n",
    "print(\"Loaded CIFAR-10 training set:\")\n",
    "print(f\" - np.shape(images)     {np.shape(images)}\")\n",
    "print(f\" - np.shape(labels)     {np.shape(labels)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the images are now in RAM in the `images` [variable](https://realpython.com/python-variables/), with their corresponding meta data in `labels`, and are ready for you to manipulate. Next, you can install the Python packages you’ll use for the three methods.\n",
    "\n",
    "**Note:** That last code block used f-strings. You can read more about them in [Python’s F-String for String Interpolation and Formatting](https://realpython.com/python-f-strings/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Storing Images on Disk[](https://realpython.com/storing-images-in-python/#setup-for-storing-images-on-disk \"Permanent link\")\n",
    "\n",
    "You’ll need to set up your environment for the default method of saving and accessing these images from disk. This article will assume you have Python 3.x installed on your system, and will use `Pillow` for the image manipulation:\n",
    "\n",
    "Shell\n",
    "\n",
    "``` sh\n",
    "pip install Pillow\n",
    "```\n",
    "\n",
    "Alternatively, if you prefer, you can install it using [Anaconda](https://anaconda.org/conda-forge/pillow):\n",
    "\n",
    "Shell\n",
    "\n",
    "``` sh\n",
    "conda install -c conda-forge pillow\n",
    "```\n",
    "\n",
    "**Note:** `PIL` is the original version of the Python Imaging Library, which is no longer maintained and is not compatible with Python 3.x. If you have previously installed `PIL`, make sure to uninstall it before installing `Pillow`, as they can’t exist together.\n",
    "\n",
    "Now you’re ready for storing and reading images from disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started With LMDB[](https://realpython.com/storing-images-in-python/#getting-started-with-lmdb \"Permanent link\")\n",
    "\n",
    "[LMDB](https://en.wikipedia.org/wiki/Lightning_Memory-Mapped_Database), sometimes referred to as the “Lightning Database,” stands for Lightning Memory-Mapped Database because it’s fast and uses memory-mapped files. It’s a key-value store, not a relational database.\n",
    "\n",
    "In terms of implementation, LMDB is a B+ tree, which basically means that it is a tree-like graph structure stored in memory where each key-value element is a node, and nodes can have many children. Nodes on the same level are linked to one another for fast traversal.\n",
    "\n",
    "Critically, key components of the B+ tree are set to correspond to the page size of the host operating system, maximizing efficiency when accessing any key-value pair in the database. Since LMDB high-performance heavily relies on this particular point, LMDB efficiency has been shown to be dependent on the underlying file system and its implementation.\n",
    "\n",
    "Another key reason for the efficiency of LMDB is that it is memory-mapped. This means that **it returns direct pointers to the memory addresses of both keys and values**, without needing to copy anything in memory as most other databases do.\n",
    "\n",
    "Those who want to dive into a bit more of the internal implementation details of B+ trees can check out [this article on B+ trees](http://www.cburch.com/cs/340/reading/btree/index.html) and then play with [this visualization of node insertion](https://www.cs.usfca.edu/~galles/visualization/BPlusTree.html).\n",
    "\n",
    "If B+ trees don’t interest you, don’t worry. You don’t need to know much about their internal implementation in order to use LMDB. We will be using the [Python binding](https://realpython.com/python-bindings-overview/) for the LMDB C library, which can be installed via pip:\n",
    "\n",
    "Shell\n",
    "\n",
    "```\n",
    "pip install lmdb\n",
    "```\n",
    "\n",
    "You also have the option of installing via [Anaconda](https://anaconda.org/conda-forge/python-lmdb):\n",
    "\n",
    "Shell\n",
    "\n",
    "```\n",
    "conda install -c conda-forge python-lmdb\n",
    "```\n",
    "\n",
    "Check that you can `import lmdb` from a Python shell, and you’re good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing a Single Image[](https://realpython.com/storing-images-in-python/#storing-a-single-image \"Permanent link\")\n",
    "\n",
    "Now that you have a general overview of the methods, let’s dive straight in and look at a quantitative comparison of the basic tasks we care about: **how long it takes to read and write files, and how much disk memory will be used.** This will also serve as a basic introduction to how the methods work, with code examples of how to use them.\n",
    "\n",
    "When I refer to “files,” I generally mean a lot of them. However, it is important to make a distinction since some methods may be optimized for different operations and quantities of files.\n",
    "\n",
    "For the purposes of experimentation, **we can compare the performance between various quantities of files, by factors of 10 from a single image to 100,000 images.** Since our five batches of CIFAR-10 add up to 50,000 images, we can use each image twice to get to 100,000 images.\n",
    "\n",
    "To prepare for the experiments, you will want to create a folder for each method, which will contain all the database files or images, and save the paths to those directories in variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "disk_dir = Path(\"data/disk/\")\n",
    "lmdb_dir = Path(\"data/lmdb/\")\n",
    "hdf5_dir = Path(\"data/hdf5/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Path` does not automatically create the folders for you unless you specifically ask it to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "disk_dir.mkdir(parents=True, exist_ok=True)\n",
    "lmdb_dir.mkdir(parents=True, exist_ok=True)\n",
    "hdf5_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can move on to running the actual experiments, with code examples of how to perform basic tasks with the three different methods. We can use the `timeit` module, which is included in the Python standard library, to help time the experiments.\n",
    "\n",
    "Although the main purpose of this article is not to learn the APIs of the different Python packages, it is helpful to have an understanding of how they can be implemented. We will go through the general principles alongside all the code used to conduct the storing experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing to Disk[](https://realpython.com/storing-images-in-python/#storing-to-disk \"Permanent link\")\n",
    "\n",
    "Our input for this experiment is a single image `image`, currently in memory as a NumPy array. You want to save it first to disk as a `.png` image, and name it using a unique image ID `image_id`. This can be done using the `Pillow` package you installed earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import csv\n",
    "\n",
    "def store_single_disk(image, image_id, label):\n",
    "    \"\"\" Stores a single image as a .png file on disk.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        image       image array, (32, 32, 3) to be stored\n",
    "        image_id    integer unique ID for image\n",
    "        label       image label\n",
    "    \"\"\"\n",
    "    Image.fromarray(image).save(disk_dir / f\"{image_id}.png\")\n",
    "\n",
    "    with open(disk_dir / f\"{image_id}.csv\", \"wt\") as csvfile:\n",
    "        writer = csv.writer(\n",
    "            csvfile, delimiter=\" \", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL\n",
    "        )\n",
    "        writer.writerow([label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This saves the image. In all realistic applications, you also care about the meta data attached to the image, which in our example dataset is the image label. When you’re storing images to disk, there are several options for saving the meta data.\n",
    "\n",
    "One solution is to encode the labels into the image name. This has the advantage of not requiring any extra files.\n",
    "\n",
    "However, it also has the big disadvantage of forcing you to deal with all the files whenever you do anything with labels. Storing the labels in a separate file allows you to play around with the labels alone, without having to load the images. Above, I have stored the labels in a separate `.csv` files for this experiment.\n",
    "\n",
    "Now let’s move on to doing the exact same task with LMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CIFAR_Image:\n",
    "    def __init__(self, image, label):\n",
    "        # Dimensions of image for reconstruction - not really necessary \n",
    "        # for this dataset, but some datasets may include images of \n",
    "        # varying sizes\n",
    "        self.channels = image.shape[2]\n",
    "        self.size = image.shape[:2]\n",
    "\n",
    "        self.image = image.tobytes()\n",
    "        self.label = label\n",
    "\n",
    "    def get_image(self):\n",
    "        \"\"\" Returns the image as a numpy array. \"\"\"\n",
    "        image = np.frombuffer(self.image, dtype=np.uint8)\n",
    "        return image.reshape(*self.size, self.channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, because LMDB is memory-mapped, new databases need to know how much memory they are expected to use up. This is relatively straightforward in our case, but it can be a massive pain in other cases, which you will see in more depth in a later section. LMDB calls this variable the `map_size`.\n",
    "\n",
    "Finally, read and write operations with LMDB are performed in `transactions`. You can think of them as similar to those of a traditional database, consisting of a group of operations on the database. This may look already significantly more complicated than the disk version, but hang on and keep reading!\n",
    "\n",
    "With those three points in mind, let’s look at the code to save a single image to a LMDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import lmdb\n",
    "import pickle\n",
    "\n",
    "def store_single_lmdb(image, image_id, label):\n",
    "    \"\"\" Stores a single image to a LMDB.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        image       image array, (32, 32, 3) to be stored\n",
    "        image_id    integer unique ID for image\n",
    "        label       image label\n",
    "    \"\"\"\n",
    "    map_size = image.nbytes * 10\n",
    "\n",
    "    # Create a new LMDB environment\n",
    "    env = lmdb.open(str(lmdb_dir / f\"single_lmdb\"), map_size=map_size)\n",
    "\n",
    "    # Start a new write transaction\n",
    "    with env.begin(write=True) as txn:\n",
    "        # All key-value pairs need to be strings\n",
    "        value = CIFAR_Image(image, label)\n",
    "        key = f\"{image_id:08}\"\n",
    "        txn.put(key.encode(\"ascii\"), pickle.dumps(value))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** It’s a good idea to calculate the exact number of bytes each key-value pair will take up.\n",
    "\n",
    "With a dataset of images of varying size, this will be an approximation, but you can use `sys.getsizeof()` to get a reasonable approximation. Keep in mind that `sys.getsizeof(CIFAR_Image)` will only return the size of a class definition, which is 1056, _not_ the size of an instantiated object.\n",
    "\n",
    "The function will also not be able to fully calculate nested items, [lists](https://realpython.com/python-lists-tuples/), or objects containing references to other objects.\n",
    "\n",
    "Alternately, you could use [`pympler`](https://pythonhosted.org/Pympler/#usage-examples) to save you some calculations by determining the exact size of an object.\n",
    "\n",
    "You are now ready to save an image to LMDB. Lastly, let’s look at the final method, HDF5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing With HDF5[](https://realpython.com/storing-images-in-python/#storing-with-hdf5 \"Permanent link\")\n",
    "\n",
    "Remember that an HDF5 file can contain more than one dataset. In this rather trivial case, you can create two datasets, one for the image, and one for its meta data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import h5py\n",
    "\n",
    "def store_single_hdf5(image, image_id, label):\n",
    "    \"\"\" Stores a single image to an HDF5 file.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        image       image array, (32, 32, 3) to be stored\n",
    "        image_id    integer unique ID for image\n",
    "        label       image label\n",
    "    \"\"\"\n",
    "    # Create a new HDF5 file\n",
    "    file = h5py.File(hdf5_dir / f\"{image_id}.h5\", \"w\")\n",
    "\n",
    "    # Create a dataset in the file\n",
    "    dataset = file.create_dataset(\n",
    "        \"image\", np.shape(image), h5py.h5t.STD_U8BE, data=image\n",
    "    )\n",
    "    meta_set = file.create_dataset(\n",
    "        \"meta\", np.shape(label), h5py.h5t.STD_U8BE, data=label\n",
    "    )\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`h5py.h5t.STD_U8BE` specifies the type of data that will be stored in the dataset, which in this case is unsigned 8-bit integers. You can see a full [list of HDF’s predefined datatypes here](http://api.h5py.org/h5t.html).\n",
    "\n",
    "**Note:** The choice of datatype will strongly affect the runtime and storage requirements of HDF5, so it is best to choose your minimum requirements.\n",
    "\n",
    "Now that we have reviewed the three methods of saving a single image, let’s move on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments for Storing a Single Image[](https://realpython.com/storing-images-in-python/#experiments-for-storing-a-single-image \"Permanent link\")\n",
    "\n",
    "Now you can put all three functions for saving a single image into a [dictionary](https://realpython.com/python-dicts/), which can be called later during the timing experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_store_single_funcs = dict(\n",
    "    disk=store_single_disk, lmdb=store_single_lmdb, hdf5=store_single_hdf5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, everything is ready for conducting the timed experiment. Let’s try saving the first image from CIFAR and its corresponding label, and storing it in the three different ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: disk, Time usage: 0.0340491000097245\n",
      "Method: lmdb, Time usage: 0.010862199997063726\n",
      "Method: hdf5, Time usage: 0.0009256000048480928\n"
     ]
    }
   ],
   "source": [
    "from timeit import timeit\n",
    "\n",
    "store_single_timings = dict()\n",
    "\n",
    "for method in (\"disk\", \"lmdb\", \"hdf5\"):\n",
    "    t = timeit(\n",
    "        \"_store_single_funcs[method](image, 0, label)\",\n",
    "        setup=\"image=images[0]; label=labels[0]\",\n",
    "        number=1,\n",
    "        globals=globals(),\n",
    "    )\n",
    "    store_single_timings[method] = t\n",
    "    print(f\"Method: {method}, Time usage: {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** While you’re playing around with LMDB, you may see a `MapFullError: mdb_txn_commit: MDB_MAP_FULL: Environment mapsize limit reached` error. It’s important to note that LMDB does **not** overwrite preexisting values, even if they have the same key.\n",
    "\n",
    "This contributes to the fast write time, but it also means that if you store an image more than once in the same LMDB file, then you will use up the map size. If you run a store function, be sure to delete any preexisting LMDB files first.\n",
    "\n",
    "Remember that we’re interested in runtime, displayed here in seconds, and also the memory usage:\n",
    "\n",
    "|Method|Save Single Image + Meta|Memory|\n",
    "|---|---|---|\n",
    "|Disk|1.915 ms|8 K|\n",
    "|LMDB|1.203 ms|32 K|\n",
    "|HDF5|8.243 ms|8 K|\n",
    "\n",
    "There are two takeaways here:\n",
    "\n",
    "1. All of the methods are trivially quick.\n",
    "2. In terms of disk usage, LMDB uses more.\n",
    "\n",
    "Clearly, despite LMDB having a slight performance lead, we haven’t convinced anyone why to not just store images on disk. After all, it’s a human readable format, and you can open and view them from any file system browser! Well, it’s time to look at a lot more images…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Many Images[](https://realpython.com/storing-images-in-python/#storing-many-images \"Permanent link\")\n",
    "\n",
    "You have seen the code for using the various storage methods to save a single image, so now we need to adjust the code to save many images and then run the timed experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Adjusting the Code for Many Images[](https://realpython.com/storing-images-in-python/#adjusting-the-code-for-many-images \"Permanent link\")\n",
    "\n",
    "Saving _multiple_ images as `.png` files is as straightforward as calling `store_single_method()` multiple times. But this isn’t true for LMDB or HDF5, since you don’t want a different database file for each image. Rather, you want to put all of the images into one or more files.\n",
    "\n",
    "You will need to slightly alter the code and create three new functions that accept multiple images, `store_many_disk()`, `store_many_lmdb()`, and `store_many_hdf5`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_many_disk(images, labels):\n",
    "    \"\"\" Stores an array of images to disk\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        images       images array, (N, 32, 32, 3) to be stored\n",
    "        labels       labels array, (N, 1) to be stored\n",
    "    \"\"\"\n",
    "    num_images = len(images)\n",
    "\n",
    "    # Save all the images one by one\n",
    "    for i, image in enumerate(images):\n",
    "        Image.fromarray(image).save(disk_dir / f\"{i}.png\")\n",
    "\n",
    "    # Save all the labels to the csv file\n",
    "    with open(disk_dir / f\"{num_images}.csv\", \"w\") as csvfile:\n",
    "        writer = csv.writer(\n",
    "            csvfile, delimiter=\" \", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL\n",
    "        )\n",
    "        for label in labels:\n",
    "            # This typically would be more than just one value per row\n",
    "            writer.writerow([label])\n",
    "\n",
    "def store_many_lmdb(images, labels):\n",
    "    \"\"\" Stores an array of images to LMDB.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        images       images array, (N, 32, 32, 3) to be stored\n",
    "        labels       labels array, (N, 1) to be stored\n",
    "    \"\"\"\n",
    "    num_images = len(images)\n",
    "\n",
    "    map_size = num_images * images[0].nbytes * 10\n",
    "\n",
    "    # Create a new LMDB DB for all the images\n",
    "    env = lmdb.open(str(lmdb_dir / f\"{num_images}_lmdb\"), map_size=map_size)\n",
    "\n",
    "    # Same as before — but let's write all the images in a single transaction\n",
    "    with env.begin(write=True) as txn:\n",
    "        for i in range(num_images):\n",
    "            # All key-value pairs need to be Strings\n",
    "            value = CIFAR_Image(images[i], labels[i])\n",
    "            key = f\"{i:08}\"\n",
    "            txn.put(key.encode(\"ascii\"), pickle.dumps(value))\n",
    "    env.close()\n",
    "\n",
    "def store_many_hdf5(images, labels):\n",
    "    \"\"\" Stores an array of images to HDF5.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        images       images array, (N, 32, 32, 3) to be stored\n",
    "        labels       labels array, (N, 1) to be stored\n",
    "    \"\"\"\n",
    "    num_images = len(images)\n",
    "\n",
    "    # Create a new HDF5 file\n",
    "    file = h5py.File(hdf5_dir / f\"{num_images}_many.h5\", \"w\")\n",
    "\n",
    "    # Create a dataset in the file\n",
    "    dataset = file.create_dataset(\n",
    "        \"images\", np.shape(images), h5py.h5t.STD_U8BE, data=images\n",
    "    )\n",
    "    meta_set = file.create_dataset(\n",
    "        \"meta\", np.shape(labels), h5py.h5t.STD_U8BE, data=labels\n",
    "    )\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you could store more than one file to disk, the image files method was altered to loop over each image in the list. For LMDB, a loop is also needed since we are creating a `CIFAR_Image` object for each image and its meta data.\n",
    "\n",
    "The smallest adjustment is with the HDF5 method. In fact, there’s hardly an adjustment at all! HFD5 files have no limitation on file size aside from external restrictions or dataset size, so all the images were stuffed into a single dataset, just like before.\n",
    "\n",
    "Next, you will need to prepare the dataset for the experiments by increasing its size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Dataset[](https://realpython.com/storing-images-in-python/#preparing-the-dataset \"Permanent link\")\n",
    "\n",
    "Before running the experiments again, let’s first double our dataset size so that we can test with up to 100,000 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 32, 32, 3)\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cutoffs = [10, 100, 1000, 10000, 100000]\n",
    "\n",
    "# Let's double our images so that we have 100,000\n",
    "images = np.concatenate((images, images), axis=0)\n",
    "labels = np.concatenate((labels, labels), axis=0)\n",
    "\n",
    "# Make sure you actually have 100,000 images and labels\n",
    "print(np.shape(images))\n",
    "print(np.shape(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that there are enough images, it’s time for the experiment.\n",
    "\n",
    "### Experiment for Storing Many Images[](https://realpython.com/storing-images-in-python/#experiment-for-storing-many-images \"Permanent link\")\n",
    "\n",
    "As you did with reading many images, you can create a dictionary handling all the functions with `store_many_` and run the experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: disk, Time usage: 0.005365499993786216\n",
      "Method: lmdb, Time usage: 0.005308400010108016\n",
      "Method: hdf5, Time usage: 0.0009045000042533502\n",
      "Method: disk, Time usage: 0.032640499994158745\n",
      "Method: lmdb, Time usage: 0.006935099998372607\n",
      "Method: hdf5, Time usage: 0.0009580999903846532\n",
      "Method: disk, Time usage: 0.31937180100067053\n",
      "Method: lmdb, Time usage: 0.024016100011067465\n",
      "Method: hdf5, Time usage: 0.002382300008321181\n",
      "Method: disk, Time usage: 3.2408651019941317\n",
      "Method: lmdb, Time usage: 0.1679007999919122\n",
      "Method: hdf5, Time usage: 0.018682599999010563\n",
      "Method: disk, Time usage: 32.49853784698644\n",
      "Method: lmdb, Time usage: 1.9063706029992318\n",
      "Method: hdf5, Time usage: 0.16321090000565164\n"
     ]
    }
   ],
   "source": [
    "_store_many_funcs = dict(\n",
    "    disk=store_many_disk, lmdb=store_many_lmdb, hdf5=store_many_hdf5\n",
    ")\n",
    "\n",
    "from timeit import timeit\n",
    "\n",
    "store_many_timings = {\"disk\": [], \"lmdb\": [], \"hdf5\": []}\n",
    "\n",
    "for cutoff in cutoffs:\n",
    "    for method in (\"disk\", \"lmdb\", \"hdf5\"):\n",
    "        t = timeit(\n",
    "            \"_store_many_funcs[method](images_, labels_)\",\n",
    "            setup=\"images_=images[:cutoff]; labels_=labels[:cutoff]\",\n",
    "            number=1,\n",
    "            globals=globals(),\n",
    "        )\n",
    "        store_many_timings[method].append(t)\n",
    "\n",
    "        # Print out the method, cutoff, and elapsed time\n",
    "        print(f\"Method: {method}, Time usage: {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you’re following along and running the code yourself, you’ll need to sit back a moment in suspense and wait for 111,110 images to be stored three times each to your disk, in three different formats. You’ll also need to say goodbye to approximately 2 GB of disk space.\n",
    "\n",
    "Now for the moment of truth! **How long did all of that storing take?** A picture is worth a thousand words:\n",
    "\n",
    "[![store-many](https://files.realpython.com/media/store_many.273573157770.png)](https://files.realpython.com/media/store_many.273573157770.png)\n",
    "\n",
    "[![store-many-log](https://files.realpython.com/media/store_many_log.29e8ae980ab6.png)](https://files.realpython.com/media/store_many_log.29e8ae980ab6.png)\n",
    "\n",
    "The first graph shows the normal, unadjusted storage time, highlighting the drastic difference between storing to `.png` files and LMDB or HDF5.\n",
    "\n",
    "The second graph shows the `log` of the timings, highlighting that HDF5 starts out slower than LMDB but, with larger quantities of images, comes out slightly ahead.\n",
    "\n",
    "While exact results may vary depending on your machine, **this is why LMDB and HDF5 are worth thinking about.** Here’s the code that generated the above graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_with_legend(\n",
    "    x_range, y_data, legend_labels, x_label, y_label, title, log=False\n",
    "):\n",
    "    \"\"\" Displays a single plot with multiple datasets and matching legends.\n",
    "        Parameters:\n",
    "        --------------\n",
    "        x_range         list of lists containing x data\n",
    "        y_data          list of lists containing y values\n",
    "        legend_labels   list of string legend labels\n",
    "        x_label         x axis label\n",
    "        y_label         y axis label\n",
    "    \"\"\"\n",
    "    plt.style.use(\"seaborn-whitegrid\")\n",
    "    plt.figure(figsize=(10, 7))\n",
    "\n",
    "    if len(y_data) != len(legend_labels):\n",
    "        raise TypeError(\n",
    "            \"Error: number of data sets does not match number of labels.\"\n",
    "        )\n",
    "\n",
    "    all_plots = []\n",
    "    for data, label in zip(y_data, legend_labels):\n",
    "        if log:\n",
    "            temp, = plt.loglog(x_range, data, label=label)\n",
    "        else:\n",
    "            temp, = plt.plot(x_range, data, label=label)\n",
    "        all_plots.append(temp)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend(handles=all_plots)\n",
    "    plt.show()\n",
    "\n",
    "# Getting the store timings data to display\n",
    "disk_x = store_many_timings[\"disk\"]\n",
    "lmdb_x = store_many_timings[\"lmdb\"]\n",
    "hdf5_x = store_many_timings[\"hdf5\"]\n",
    "\n",
    "plot_with_legend(\n",
    "    cutoffs,\n",
    "    [disk_x, lmdb_x, hdf5_x],\n",
    "    [\"PNG files\", \"LMDB\", \"HDF5\"],\n",
    "    \"Number of images\",\n",
    "    \"Seconds to store\",\n",
    "    \"Storage time\",\n",
    "    log=False,\n",
    ")\n",
    "\n",
    "plot_with_legend(\n",
    "    cutoffs,\n",
    "    [disk_x, lmdb_x, hdf5_x],\n",
    "    [\"PNG files\", \"LMDB\", \"HDF5\"],\n",
    "    \"Number of images\",\n",
    "    \"Seconds to store\",\n",
    "    \"Log storage time\",\n",
    "    log=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s go on to reading the images back out.\n",
    "\n",
    "## Reading a Single Image[](https://realpython.com/storing-images-in-python/#reading-a-single-image \"Permanent link\")\n",
    "\n",
    "First, let’s consider the case for reading a single image back into an array for each of the three methods.\n",
    "\n",
    "### Reading From Disk[](https://realpython.com/storing-images-in-python/#reading-from-disk \"Permanent link\")\n",
    "\n",
    "Of the three methods, LMDB requires the most legwork when reading image files back out of memory, because of the serialization step. Let’s walk through these functions that read a single image out for each of the three storage formats.\n",
    "\n",
    "First, read a single image and its meta from a `.png` and `.csv` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_disk(image_id):\n",
    "    \"\"\" Stores a single image to disk.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        image_id    integer unique ID for image\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        image       image array, (32, 32, 3) to be stored\n",
    "        label       associated meta data, int label\n",
    "    \"\"\"\n",
    "    image = np.array(Image.open(disk_dir / f\"{image_id}.png\"))\n",
    "\n",
    "    with open(disk_dir / f\"{image_id}.csv\", \"r\") as csvfile:\n",
    "        reader = csv.reader(\n",
    "            csvfile, delimiter=\" \", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL\n",
    "        )\n",
    "        label = int(next(reader)[0])\n",
    "\n",
    "    return image, label\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading From LMDB[](https://realpython.com/storing-images-in-python/#reading-from-lmdb \"Permanent link\")\n",
    "\n",
    "Next, read the same image and meta from an LMDB by opening the environment and starting a read transaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_single_lmdb(image_id):\n",
    "    \"\"\" Stores a single image to LMDB.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        image_id    integer unique ID for image\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        image       image array, (32, 32, 3) to be stored\n",
    "        label       associated meta data, int label\n",
    "    \"\"\"\n",
    "    # Open the LMDB environment\n",
    "    env = lmdb.open(str(lmdb_dir / f\"single_lmdb\"), readonly=True)\n",
    "\n",
    "    # Start a new read transaction\n",
    "    with env.begin() as txn:\n",
    "        # Encode the key the same way as we stored it\n",
    "        data = txn.get(f\"{image_id:08}\".encode(\"ascii\"))\n",
    "        # Remember it's a CIFAR_Image object that is loaded\n",
    "        cifar_image = pickle.loads(data)\n",
    "        # Retrieve the relevant bits\n",
    "        image = cifar_image.get_image()\n",
    "        label = cifar_image.label\n",
    "    env.close()\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a couple points to not about the code snippet above:\n",
    "\n",
    "- **Line 13:** The `readonly=True` flag specifies that no writes will be allowed on the LMDB file until the transaction is finished. In database lingo, it’s equivalent to taking a read lock.\n",
    "- **Line 20:** To retrieve the CIFAR_Image object, you need to reverse the steps we took to pickle it when we were writing it. This is where the `get_image()` of the object is helpful.\n",
    "\n",
    "This wraps up reading the image back out from LMDB. Finally, you will want to do the same with HDF5.\n",
    "\n",
    "### Reading From HDF5[](https://realpython.com/storing-images-in-python/#reading-from-hdf5 \"Permanent link\")\n",
    "\n",
    "Reading from HDF5 looks very similar to the writing process. Here is the code to open and read the HDF5 file and parse the same image and meta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_hdf5(image_id):\n",
    "    \"\"\" Stores a single image to HDF5.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        image_id    integer unique ID for image\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        image       image array, (32, 32, 3) to be stored\n",
    "        label       associated meta data, int label\n",
    "    \"\"\"\n",
    "    # Open the HDF5 file\n",
    "    file = h5py.File(hdf5_dir / f\"{image_id}.h5\", \"r+\")\n",
    "\n",
    "    image = np.array(file[\"/image\"]).astype(\"uint8\")\n",
    "    label = int(np.array(file[\"/meta\"]).astype(\"uint8\"))\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you access the various datasets in the file by indexing the `file` object using the dataset name preceded by a forward slash `/`. As before, you can create a dictionary containing all the read functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_read_single_funcs = dict(\n",
    "    disk=read_single_disk, lmdb=read_single_lmdb, hdf5=read_single_hdf5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this dictionary prepared, you are ready for running the experiment.\n",
    "\n",
    "### Experiment for Reading a Single Image[](https://realpython.com/storing-images-in-python/#experiment-for-reading-a-single-image \"Permanent link\")\n",
    "\n",
    "You might expect that the experiment for reading a single image in will have somewhat trivial results, but here’s the experiment code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit\n",
    "\n",
    "read_single_timings = dict()\n",
    "\n",
    "for method in (\"disk\", \"lmdb\", \"hdf5\"):\n",
    "    t = timeit(\n",
    "        \"_read_single_funcs[method](0)\",\n",
    "        setup=\"image=images[0]; label=labels[0]\",\n",
    "        number=1,\n",
    "        globals=globals(),\n",
    "    )\n",
    "    read_single_timings[method] = t\n",
    "    print(f\"Method: {method}, Time usage: {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results of the experiment for reading a single image:\n",
    "\n",
    "|Method|Read Single Image + Meta|\n",
    "|---|---|\n",
    "|Disk|1.61970 ms|\n",
    "|LMDB|4.52063 ms|\n",
    "|HDF5|1.98036 ms|\n",
    "\n",
    "It’s slightly faster to read the `.png` and `.csv` files directly from disk, but all three methods perform trivially quickly. The experiments we’ll do next are much more interesting.\n",
    "\n",
    "## Reading Many Images[](https://realpython.com/storing-images-in-python/#reading-many-images \"Permanent link\")\n",
    "\n",
    "Now you can adjust the code to read many images at once. This is likely the action you’ll be performing most often, so the runtime performance is essential.\n",
    "\n",
    "[Remove ads](https://realpython.com/account/join/)\n",
    "\n",
    "### Adjusting the Code for Many Images[](https://realpython.com/storing-images-in-python/#adjusting-the-code-for-many-images_1 \"Permanent link\")\n",
    "\n",
    "Extending the functions above, you can create functions with `read_many_`, which can be used for the next experiments. Like before, it is interesting to compare performance when reading different quantities of images, which are repeated in the code below for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_many_disk(num_images):\n",
    "    \"\"\" Reads image from disk.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        num_images   number of images to read\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        images      images array, (N, 32, 32, 3) to be stored\n",
    "        labels      associated meta data, int label (N, 1)\n",
    "    \"\"\"\n",
    "    images, labels = [], []\n",
    "\n",
    "    # Loop over all IDs and read each image in one by one\n",
    "    for image_id in range(num_images):\n",
    "        images.append(np.array(Image.open(disk_dir / f\"{image_id}.png\")))\n",
    "\n",
    "    with open(disk_dir / f\"{num_images}.csv\", \"r\") as csvfile:\n",
    "        reader = csv.reader(\n",
    "            csvfile, delimiter=\" \", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL\n",
    "        )\n",
    "        for row in reader:\n",
    "            labels.append(int(row[0]))\n",
    "    return images, labels\n",
    "\n",
    "def read_many_lmdb(num_images):\n",
    "    \"\"\" Reads image from LMDB.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        num_images   number of images to read\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        images      images array, (N, 32, 32, 3) to be stored\n",
    "        labels      associated meta data, int label (N, 1)\n",
    "    \"\"\"\n",
    "    images, labels = [], []\n",
    "    env = lmdb.open(str(lmdb_dir / f\"{num_images}_lmdb\"), readonly=True)\n",
    "\n",
    "    # Start a new read transaction\n",
    "    with env.begin() as txn:\n",
    "        # Read all images in one single transaction, with one lock\n",
    "        # We could split this up into multiple transactions if needed\n",
    "        for image_id in range(num_images):\n",
    "            data = txn.get(f\"{image_id:08}\".encode(\"ascii\"))\n",
    "            # Remember that it's a CIFAR_Image object \n",
    "            # that is stored as the value\n",
    "            cifar_image = pickle.loads(data)\n",
    "            # Retrieve the relevant bits\n",
    "            images.append(cifar_image.get_image())\n",
    "            labels.append(cifar_image.label)\n",
    "    env.close()\n",
    "    return images, labels\n",
    "\n",
    "def read_many_hdf5(num_images):\n",
    "    \"\"\" Reads image from HDF5.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        num_images   number of images to read\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        images      images array, (N, 32, 32, 3) to be stored\n",
    "        labels      associated meta data, int label (N, 1)\n",
    "    \"\"\"\n",
    "    images, labels = [], []\n",
    "\n",
    "    # Open the HDF5 file\n",
    "    file = h5py.File(hdf5_dir / f\"{num_images}_many.h5\", \"r+\")\n",
    "\n",
    "    images = np.array(file[\"/images\"]).astype(\"uint8\")\n",
    "    labels = np.array(file[\"/meta\"]).astype(\"uint8\")\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "_read_many_funcs = dict(\n",
    "    disk=read_many_disk, lmdb=read_many_lmdb, hdf5=read_many_hdf5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the reading functions stored in a dictionary as with the writing functions, you’re all set for the experiment.\n",
    "\n",
    "### Experiment for Reading Many Images[](https://realpython.com/storing-images-in-python/#experiment-for-reading-many-images \"Permanent link\")\n",
    "\n",
    "You can now run the experiment for reading many images out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit\n",
    "\n",
    "read_many_timings = {\"disk\": [], \"lmdb\": [], \"hdf5\": []}\n",
    "\n",
    "for cutoff in cutoffs:\n",
    "    for method in (\"disk\", \"lmdb\", \"hdf5\"):\n",
    "        t = timeit(\n",
    "            \"_read_many_funcs[method](num_images)\",\n",
    "            setup=\"num_images=cutoff\",\n",
    "            number=1,\n",
    "            globals=globals(),\n",
    "        )\n",
    "        read_many_timings[method].append(t)\n",
    "\n",
    "        # Print out the method, cutoff, and elapsed time\n",
    "        print(f\"Method: {method}, No. images: {cutoff}, Time usage: {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did previously, you can graph the read experiment results:\n",
    "\n",
    "[![read-many-image](https://files.realpython.com/media/read_many.9c4a6dc5bdc0.png)](https://files.realpython.com/media/read_many.9c4a6dc5bdc0.png)\n",
    "\n",
    "[![read-many-log](https://files.realpython.com/media/read_many_log.594dac8746ad.png)](https://files.realpython.com/media/read_many_log.594dac8746ad.png)\n",
    "\n",
    "The top graph shows the normal, unadjusted read times, showing the drastic difference between reading from `.png` files and LMDB or HDF5.\n",
    "\n",
    "In contrast, the graph on the bottom shows the `log` of the timings, highlighting the relative differences with fewer images. Namely, we can see how HDF5 starts out behind but, with more images, becomes consistently faster than LMDB by a small margin.\n",
    "\n",
    "Plot the Read TimingsShow/Hide\n",
    "\n",
    "**In practice, the write time is often less critical than the read time.** Imagine that you are training a deep neural network on images, and only half of your entire image dataset fits into RAM at once. Each epoch of training a network requires the entire dataset, and the model needs a few hundred epochs to converge. You will essentially be reading half of the dataset into memory every epoch.\n",
    "\n",
    "There are several tricks people do, such as training pseudo-epochs to make this slightly better, but you get the idea.\n",
    "\n",
    "Now, look again at the read graph above. The difference between a 40-second and 4-second read time suddenly is the difference between waiting six hours for your model to train, or forty minutes!\n",
    "\n",
    "If we view the read and write times on the same chart, we have the following:\n",
    "\n",
    "[![read-write](https://files.realpython.com/media/read_write.a4f87d39489d.png)](https://files.realpython.com/media/read_write.a4f87d39489d.png)\n",
    "\n",
    "Plot the Read and Write TimingsShow/Hide\n",
    "\n",
    "When you’re storing images as `.png` files, there is a big difference between write and read times. However, with LMDB and HDF5, the difference is much less marked. Overall, even if read time is more critical than write time, there is a strong argument for storing images using LMDB or HDF5.\n",
    "\n",
    "Now that you’ve seen the performance benefits of LMDB and HDF5, let’s look at another crucial metric: disk usage.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
